{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0YCqep0CIzA"
   },
   "source": [
    "# **Deep Image Colorization: A ResNet-Based U-Net and PatchGAN Approach with Perceptual Loss**\n",
    "\n",
    "#### **Author: Constantino Harry Alexander (25206605)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIyNkp1XnR1Q"
   },
   "source": [
    "## **Project Overview**\n",
    "\n",
    "This project implements a conditional Generative Adversarial Network (cGAN) for automatic image colorization. The architecture is inspired by the Image-to-Image Translation framework (Pix2Pix) by Isola et al. [1].\n",
    "\n",
    "### **Key Architectural Features:**\n",
    "\n",
    "- Generator: A U-Net architecture [2] with a ResNet-18 backbone pre-trained on ImageNet. This allows the model to leverage rich semantic feature extraction (e.g., recognizing \"sky\" or \"grass\") rather than learning from scratch.\n",
    "- Discriminator: A PatchGAN discriminator [1] which penalizes structure at the scale of local image patches, ensuring high-frequency sharpness.\n",
    "- Loss Function: A hybrid loss combining L1 (pixel-level), Adversarial (GAN), and Perceptual (LPIPS) losses to solve the \"sepia effect\" common in regression-only models.\n",
    "\n",
    "###**References:**\n",
    "\n",
    "- [1] Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. CVPR.\n",
    "- [2] Ronneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. MICCAI.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5rmQJSxoXre"
   },
   "source": [
    "#**Environment Setup and Data Aquisition üíªüóÇÔ∏è**\n",
    "\n",
    "This cell handles the initial setup of the environment. It installs necessary libraries such as lpips (for perceptual loss) and faiss-cpu (for efficient similarity search). It also configures the computation device (CUDA GPU or CPU) and downloads the \"Image Colorization Dataset\" directly from Kaggle using the Kaggle API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "id": "23OzaJC8gE2Y",
    "outputId": "0e742eb2-1608-43ec-90dd-a90f5839aed4"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio scikit-image matplotlib tqdm lpips faiss-cpu --quiet\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "from tqdm.notebook import tqdm\n",
    "import lpips  # For perceptual loss\n",
    "import faiss  # For retrieval\n",
    "\n",
    "# Suppress all warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# --- Download Dataset (If not already downloaded) ---\n",
    "if not os.path.exists('/content/dataset'):\n",
    "    !pip install -q kaggle\n",
    "    from google.colab import files\n",
    "    # Upload kaggle.json if you haven't yet\n",
    "    if not os.path.exists('kaggle.json'):\n",
    "        print(\"Please upload kaggle.json\")\n",
    "        files.upload()\n",
    "    !mkdir -p ~/.kaggle\n",
    "    !cp kaggle.json ~/.kaggle/\n",
    "    !chmod 600 ~/.kaggle/kaggle.json\n",
    "    !kaggle datasets download -d aayush9753/image-colorization-dataset\n",
    "    !unzip -q image-colorization-dataset.zip -d /content/dataset\n",
    "print(\"Dataset Downloaded and Unzipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZE9UD-JzJOK"
   },
   "source": [
    "# **Data Processing and Resizingüìä**\n",
    "Before training, raw images must be standardized. This cell iterates through the downloaded dataset, locates the training images, and resizes them to a fixed resolution of 256x256 pixels. The processed images are saved to a temporary directory to speed up loading times during the training phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 98,
     "referenced_widgets": [
      "50ff802587704cc1a461f8fb281ca961",
      "73a14d4efdf941e0a6e8d7caff55ee2c",
      "0803e60a5178460f8f1a264af90fe250",
      "c9332e0495014425af51d3ea7b423db5",
      "10f10c6aa259449cb7d17ac47b72e2e4",
      "e4b6f3e583d247a0a723c8db23a6a09a",
      "7df4f34d997146c4998dfb1939391e12",
      "195743ba0c0d40bf8e3eeee20fe68536",
      "3a00f2f7ac214666afe8972030c91978",
      "3da7e04c0d184fa9b9149631a8a00b27",
      "fd527d74428546078e0829e72e7d148d"
     ]
    },
    "id": "ipGnk_aPgKWj",
    "outputId": "404334e9-9440-4096-9172-86f15bb47e6a"
   },
   "outputs": [],
   "source": [
    "# Dataset Source: Image Colorization Dataset from Kaggle\n",
    "\n",
    "# Preprocessing Pipeline\n",
    "# 1. Image Loading: Load RGB images\n",
    "# 2. Color Space Conversion: RGB ‚Üí Lab, Normalize L [-1,1], ab [-1,1]\n",
    "# 3. Data Augmentation (Training): Stronger for robustness - added Gaussian noise, color jitter++\n",
    "\n",
    "# Train/Validation Split: 80/10/10\n",
    "\n",
    "# Why Lab: Separates luminance from color\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(\"Finding dataset location...\")\n",
    "\n",
    "possible_color_dirs = [\n",
    "    \"/content/dataset/train_color\",\n",
    "    \"/content/dataset/image-colorization-dataset/train_color\",\n",
    "    \"/content/dataset/dataset/train_color\",\n",
    "    \"/content/image-colorization-dataset/train_color\",\n",
    "]\n",
    "\n",
    "color_dir = None\n",
    "for path in possible_color_dirs:\n",
    "    if os.path.exists(path) and len(glob.glob(path + \"/*.jpg\")) > 100:\n",
    "        color_dir = path\n",
    "        break\n",
    "\n",
    "if color_dir is None:\n",
    "    matches = !find /content -type d -name \"train_color\" 2>/dev/null\n",
    "    if matches:\n",
    "        color_dir = matches[0]\n",
    "    else:\n",
    "        raise FileNotFoundError(\"Could not find train_color folder!\")\n",
    "\n",
    "os.makedirs(\"/content/processed/images\", exist_ok=True)\n",
    "\n",
    "color_paths = sorted(glob.glob(color_dir + \"/*.jpg\"))\n",
    "np.random.seed(42)\n",
    "np.random.shuffle(color_paths)\n",
    "color_paths = color_paths[:1500]  # Keep 1500 for memory\n",
    "\n",
    "print(f\"Processing {len(color_paths)} images...\")\n",
    "\n",
    "for color_path in tqdm(color_paths, desc=\"Resizing images\"):\n",
    "    try:\n",
    "        img = cv2.imread(color_path)\n",
    "        if img is not None:\n",
    "            img = cv2.resize(img, (256, 256))  # Keep 256x256\n",
    "            processed_path = \"/content/processed/images/\" + os.path.basename(color_path)\n",
    "            cv2.imwrite(processed_path, img)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(\"Processed images saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r-jJJq_NpDn9"
   },
   "source": [
    "#**Custom Dataset and Lab Color Space Conversion üìä**\n",
    "\n",
    "Here we define the ColorizationDataset class. This is a crucial step where RGB images are converted into the CIELAB (Lab) color space. Following the methodology of Zhang et al. [3], we convert RGB images into the CIELAB (Lab) color space.\n",
    "\n",
    "- L channel (Lightness): Used as the input to the model (grayscale).\n",
    "- ab channels (Color): Used as the target (ground truth) for the model to predict.\n",
    "This cell also applies data augmentation (flipping, rotation, color jitter) to prevent overfitting and creates the DataLoaders for training, validation, and testing.\n",
    "\n",
    "### **References**\n",
    "- [3] Zhang, R., Isola, P., & Efros, A. A. (2016). Colorful image colorization. ECCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "51NdLvRkgnNN"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "from skimage.color import rgb2lab\n",
    "\n",
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, root_dir=\"/content/processed/images\", split='train', transform=None):\n",
    "        self.paths = sorted(glob.glob(root_dir + \"/*.jpg\"))\n",
    "        total = len(self.paths)\n",
    "        if split == 'train':\n",
    "            self.paths = self.paths[:int(0.8 * total)]\n",
    "        elif split == 'val':\n",
    "            self.paths = self.paths[int(0.8 * total):int(0.9 * total)]\n",
    "        elif split == 'test':\n",
    "            self.paths = self.paths[int(0.9 * total):]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        img = np.array(img)\n",
    "        lab = rgb2lab(img).transpose(2, 0, 1)\n",
    "        L = (lab[[0], ...] / 50.0) - 1.0  # [-1,1]\n",
    "        ab = lab[[1, 2], ...] / 128.0  # [-1,1]\n",
    "        return {'L': torch.from_numpy(L).float(), 'ab': torch.from_numpy(ab).float(), 'path': self.paths[idx]}\n",
    "\n",
    "# Enhanced Augmentations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3),\n",
    "    transforms.RandomResizedCrop(256, scale=(0.8, 1.0)),\n",
    "    transforms.GaussianBlur(kernel_size=3),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "])\n",
    "\n",
    "# Re-initialize your datasets and loaders after running this cell\n",
    "train_dataset = ColorizationDataset(split='train', transform=train_transform)\n",
    "val_dataset = ColorizationDataset(split='val', transform=val_transform)\n",
    "test_dataset = ColorizationDataset(split='test', transform=val_transform)\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BoT9ye2mKddW"
   },
   "source": [
    "#**Model Architecture (ResNet-Based U-Net & PatchGAN) ü§ñüß†**\n",
    "\n",
    "### **Generator Design (ResNet + U-Net):**\n",
    "Instead of a standard encoder, we utilize the first 18 layers of ResNet-18 [4] as the backbone. This approach, popularized by the FastAI library [5], uses \"dynamic U-Net\" construction.\n",
    "\n",
    "- Why ResNet? Deep networks suffer from vanishing gradients. ResNet's \"skip connections\" allow gradients to flow through the network easily, enabling deeper feature extraction.\n",
    "- Why U-Net? Colorization requires perfect alignment between the input grayscale edges and the output color. U-Net's long skip connections transfer spatial information directly from the encoder to the decoder, preserving fine details.\n",
    "\n",
    "###**Discriminator Design (PatchGAN):**\n",
    "We implement a PatchGAN discriminator [1]. Unlike standard GAN discriminators that output a single \"Real/Fake\" probability for the whole image, PatchGAN classifies NxN patches of the image. This encourages the generator to focus on high-frequency structural details and sharpness.\n",
    "\n",
    "##**References:**\n",
    "\n",
    "- [1] Isola, P., Zhu, J. Y., Zhou, T., & Efros, A. A. (2017). Image-to-image translation with conditional adversarial networks. CVPR.\n",
    "- [2] Ronneberger, O., Fischer, P., & Brox, T. (2015). U-net: Convolutional networks for biomedical image segmentation. MICCAI.\n",
    "- [4] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. CVPR.\n",
    "- [5] Howard, J., & Gugger, S. (2020). Fastai: A layered API for deep learning. Information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v51O-XtdH86A",
    "outputId": "d1efa50b-2070-47c8-b077-532ef6e37c41"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "\n",
    "# --- 1. Self-Attention Block ---\n",
    "# (Keep this! The ResNetUNet below needs it)\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.query_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim//8, kernel_size=1)\n",
    "        self.value_conv = nn.Conv2d(in_channels=in_dim, out_channels=in_dim, kernel_size=1)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        m_batchsize, C, width, height = x.size()\n",
    "        proj_query = self.query_conv(x).view(m_batchsize, -1, width*height).permute(0, 2, 1)\n",
    "        proj_key = self.key_conv(x).view(m_batchsize, -1, width*height)\n",
    "        energy = torch.bmm(proj_query, proj_key)\n",
    "        attention = self.softmax(energy)\n",
    "        proj_value = self.value_conv(x).view(m_batchsize, -1, width*height)\n",
    "\n",
    "        out = torch.bmm(proj_value, attention.permute(0, 2, 1))\n",
    "        out = out.view(m_batchsize, C, width, height)\n",
    "        out = self.gamma*out + x\n",
    "        return out\n",
    "\n",
    "# --- 2. Enhanced ResNet U-Net Generator ---\n",
    "class ResNetUNet(nn.Module):\n",
    "    def __init__(self, n_classes=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # Encoder (ResNet18)\n",
    "        base_model = models.resnet18(pretrained=True)\n",
    "        self.base_layers = list(base_model.children())\n",
    "\n",
    "        # Input layer: Modify to accept 1 channel (L) instead of 3 (RGB)\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[:3])\n",
    "        self.layer0[0] = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5])\n",
    "        self.layer2 = self.base_layers[5]\n",
    "        self.layer3 = self.base_layers[6]\n",
    "        self.layer4 = self.base_layers[7]\n",
    "\n",
    "        # Decoder (Upsampling)\n",
    "        self.up1 = self.unet_block(512, 256)\n",
    "        self.up2 = self.unet_block(256 + 256, 128)\n",
    "        self.up3 = self.unet_block(128 + 128, 64)\n",
    "\n",
    "        # NEW: Attention Block added at the 64-filter level\n",
    "        self.attention = SelfAttention(64)\n",
    "\n",
    "        self.up4 = self.unet_block(64 + 64, 64)\n",
    "\n",
    "        self.up5 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64 + 64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Conv2d(32, n_classes, kernel_size=3, padding=1),\n",
    "            nn.Tanh() # Output is [-1, 1] to match Lab 'ab' range\n",
    "        )\n",
    "\n",
    "    def unet_block(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        layer0 = self.layer0(x)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        # Decoder with Skip Connections\n",
    "        up1 = self.up1(layer4)\n",
    "        up2 = self.up2(torch.cat([up1, layer3], 1))\n",
    "        up3 = self.up3(torch.cat([up2, layer2], 1))\n",
    "\n",
    "        # Apply Attention\n",
    "        up3 = self.attention(up3)\n",
    "\n",
    "        up4 = self.up4(torch.cat([up3, layer1], 1))\n",
    "        up5 = self.up5(torch.cat([up4, layer0], 1))\n",
    "\n",
    "        return up5\n",
    "\n",
    "# --- 3. Initialization ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize Generator\n",
    "generator = ResNetUNet(n_classes=2).to(device)\n",
    "\n",
    "# Initialize Generator Optimizer\n",
    "# Note: Ensure you have initialized 'opt_D' (Discriminator optimizer) in your other cell!\n",
    "opt_G = torch.optim.Adam(generator.parameters(), lr=1e-4, betas=(0.5, 0.999))\n",
    "\n",
    "print(\"Enhanced Generator (with Attention) initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bnbcOyacYGAq"
   },
   "source": [
    "### **PatchDiscriminator**\n",
    "\n",
    "A discriminator that classifies patches of the image as real or fake (rather than the whole image at once), which encourages sharper high-frequency details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4phc-OLRb4jr"
   },
   "outputs": [],
   "source": [
    "# --- PatchGAN Discriminator ---\n",
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_c=3, n_filters=64, n_layers=3):\n",
    "        super().__init__()\n",
    "        model = [nn.Conv2d(input_c, n_filters, kernel_size=4, stride=2, padding=1),\n",
    "                 nn.LeakyReLU(0.2, True)]\n",
    "\n",
    "        nf_mult = 1\n",
    "        nf_mult_prev = 1\n",
    "\n",
    "        for n in range(1, n_layers):\n",
    "            nf_mult_prev = nf_mult\n",
    "            nf_mult = min(2**n, 8)\n",
    "            model += [\n",
    "                nn.Conv2d(n_filters * nf_mult_prev, n_filters * nf_mult, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "                nn.BatchNorm2d(n_filters * nf_mult),\n",
    "                nn.LeakyReLU(0.2, True)\n",
    "            ]\n",
    "\n",
    "        nf_mult_prev = nf_mult\n",
    "        nf_mult = min(2**n_layers, 8)\n",
    "\n",
    "        model += [\n",
    "            nn.Conv2d(n_filters * nf_mult_prev, n_filters * nf_mult, kernel_size=4, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(n_filters * nf_mult),\n",
    "            nn.LeakyReLU(0.2, True)\n",
    "        ]\n",
    "\n",
    "        model += [nn.Conv2d(n_filters * nf_mult, 1, kernel_size=4, stride=1, padding=1)]\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "discriminator = PatchDiscriminator(input_c=3).to(device) # Input is L (1) + ab (2) = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KG-QDJG2yfVb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from skimage.color import lab2rgb\n",
    "\n",
    "# Vectorized, batched Lab->RGB: (B,1,H,W) + (B,2,H,W) -> (B,3,H,W), [0,1], float32\n",
    "def lab_batch_to_rgb_tensor(L, ab):\n",
    "    # L, ab are torch tensors on device, ranges: L in [-1,1], ab in [-1,1]\n",
    "    # Returns RGB torch tensor on same device, shape (B,3,H,W), float32 in [0,1]\n",
    "    device = L.device\n",
    "    B, _, H, W = L.shape\n",
    "\n",
    "    # Move to CPU for skimage, build lab in (B,H,W,3)\n",
    "    L_np = ((L.detach().cpu().float() + 1.0) * 50.0).numpy()                 # (B,1,H,W) -> L* in [0,100]\n",
    "    ab_np = (ab.detach().cpu().float() * 128.0).numpy()                      # (B,2,H,W) -> a*,b* in [-128,128]\n",
    "    lab_np = np.concatenate([L_np, ab_np], axis=1)                            # (B,3,H,W)\n",
    "    lab_np = np.transpose(lab_np, (0, 2, 3, 1))                               # (B,H,W,3)\n",
    "\n",
    "    # Apply lab2rgb per image\n",
    "    rgb_list = [lab2rgb(lab_np[i]) for i in range(B)]                         # each (H,W,3) in [0,1]\n",
    "    rgb_np = np.stack(rgb_list, axis=0)                                       # (B,H,W,3)\n",
    "    rgb_np = np.transpose(rgb_np, (0, 3, 1, 2))                               # (B,3,H,W)\n",
    "\n",
    "    rgb = torch.from_numpy(rgb_np).to(device=device, dtype=torch.float32)\n",
    "    return rgb.clamp(0.0, 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJMBVtvLpqU9"
   },
   "source": [
    "#**Retrieval-Augmented Component**\n",
    "\n",
    "This cell implements an innovative feature using FAISS. It builds an index of image embeddings using a pre-trained ResNet. This allows the system to search the training set for images that are semantically similar to the input grayscale image. These \"reference\" images can potentially be used to provide color hints to the generator, improving accuracy for ambiguous objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ba8d9fd159e542f997f411c67e46db57",
      "b642265c14be4e2abb92cdced12b7bcd",
      "8e43b9bd93c1492e851c5e30cc21e001",
      "ed54c4f0067e4ebcba98f9c127641c53",
      "64278b2a54db49bfb131d984c0d739a0",
      "a4307b68d3144447a641437b5163ec6f",
      "702f33f9c5934e4cb0041635eeb97221",
      "6505cbccb7004545ab6a01e42a97c040",
      "765a06a7f09f46a3a4d6587a3ee2430e",
      "82a781614c8e48a188a7c8aa3c16509b",
      "66f586c5853c4bc9bcb13beced6bc919"
     ]
    },
    "id": "CdGjUswwDSk6",
    "outputId": "7ed29e20-b5c5-4586-8bb7-d76b4589b1a1"
   },
   "outputs": [],
   "source": [
    "# Pre-trained ResNet for embeddings (grayscale input)\n",
    "embedder = models.resnet18(pretrained=True)\n",
    "embedder.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)  # Change to 1 channel\n",
    "embedder.fc = nn.Identity()  # Remove classifier\n",
    "embedder = embedder.to(device).eval()\n",
    "\n",
    "# Build index from train grayscales (small reference set: 500 images)\n",
    "ref_paths = train_dataset.paths[:500]\n",
    "ref_embeddings = []\n",
    "ref_abs = []  # Store ab for blending\n",
    "\n",
    "with torch.no_grad():\n",
    "    for path in tqdm(ref_paths, desc=\"Building Retrieval Index\"):\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        lab = rgb2lab(np.array(img))\n",
    "        L = torch.from_numpy((lab[:,:,0]/50 -1)[None, None, ...]).float().to(device)\n",
    "        ab = torch.from_numpy(lab[:,:,1:]/128).float().permute(2,0,1)[None,...].to(device)\n",
    "        emb = embedder(L).cpu().numpy().flatten()\n",
    "        ref_embeddings.append(emb)\n",
    "        ref_abs.append(ab.cpu())\n",
    "\n",
    "ref_embeddings = np.array(ref_embeddings)\n",
    "index = faiss.IndexFlatL2(ref_embeddings.shape[1])\n",
    "index.add(ref_embeddings)\n",
    "\n",
    "def retrieve_color_hint(L):\n",
    "    with torch.no_grad():\n",
    "        embs = embedder(L).cpu().numpy()  # (B, 512)\n",
    "        _, idxs = index.search(embs, 1)  # (B, 1)\n",
    "        hint_abs = torch.cat([ref_abs[idx[0]] for idx in idxs], dim=0) * 0.2  # (B, 2, H, W)\n",
    "        return hint_abs.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ssSJiYd3pzrX"
   },
   "source": [
    "# **üîÅTraining Procedure (with TTUR, Early Stopping, Retrieval Hint)**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duTa4WjG6CuI"
   },
   "source": [
    "### **Helper Function**\n",
    "This cell defines helper functions to visualize the results during training. It converts the Lab tensors back to RGB format and displays the Input (Grayscale), Generated Output, and Ground Truth side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VW91uERy5-xx"
   },
   "outputs": [],
   "source": [
    "def show_results_resnet(generator, dataloader, num_images=5):\n",
    "    generator.eval()\n",
    "    data = next(iter(dataloader))\n",
    "    L = data['L'].to(device)\n",
    "    ab_real = data['ab'].to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_ab = generator(L)\n",
    "\n",
    "    fake_imgs = lab_batch_to_rgb_tensor(L, fake_ab)\n",
    "    real_imgs = lab_batch_to_rgb_tensor(L, ab_real)\n",
    "\n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))\n",
    "    for i in range(num_images):\n",
    "        # Grayscale\n",
    "        gray_img = (L[i].cpu().squeeze().numpy() + 1) / 2\n",
    "        axes[i, 0].imshow(gray_img, cmap='gray')\n",
    "        axes[i, 0].set_title(\"Input (Grayscale)\")\n",
    "        axes[i, 0].axis('off')\n",
    "\n",
    "        # Generated\n",
    "        gen_img = fake_imgs[i].cpu().permute(1, 2, 0).numpy()\n",
    "        axes[i, 1].imshow(gen_img)\n",
    "        axes[i, 1].set_title(\"Generated Color\")\n",
    "        axes[i, 1].axis('off')\n",
    "\n",
    "        # Ground Truth\n",
    "        real_img = real_imgs[i].cpu().permute(1, 2, 0).numpy()\n",
    "        axes[i, 2].imshow(real_img)\n",
    "        axes[i, 2].set_title(\"Ground Truth\")\n",
    "        axes[i, 2].axis('off')\n",
    "    plt.show()\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mj3IAHLAJbfs"
   },
   "source": [
    "### **Helper Function**\n",
    "\n",
    "These are helper functions for the training loop to plot debugging features during the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HN2f87Kg3KZJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import math\n",
    "import lpips\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"\n",
    "    Helper function to plot training metrics in a single row:\n",
    "    1. GAN Losses (Generator vs Discriminator)\n",
    "    2. L1 Loss (Color Accuracy)\n",
    "    3. PSNR (Signal Quality)\n",
    "    4. LPIPS (Perceptual Quality)\n",
    "    \"\"\"\n",
    "    # Changed figsize to be wider (24) and shorter (5) for a single row\n",
    "    plt.figure(figsize=(24, 5))\n",
    "\n",
    "    # Plot 1: GAN Losses\n",
    "    plt.subplot(1, 4, 1) # 1 row, 4 columns, index 1\n",
    "    plt.plot(history['G_loss'], label='G Loss', color='orange')\n",
    "    plt.plot(history['D_loss'], label='D Loss', color='blue')\n",
    "    plt.title(\"Adversarial Training Losses\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # Plot 2: L1 Loss (Lower is better)\n",
    "    plt.subplot(1, 4, 2) # 1 row, 4 columns, index 2\n",
    "    plt.plot(history['Val_L1'], label='Validation L1', color='green')\n",
    "    plt.title(\"Color Accuracy (L1 - Lower is Better)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # Plot 3: PSNR (Higher is better)\n",
    "    plt.subplot(1, 4, 3) # 1 row, 4 columns, index 3\n",
    "    plt.plot(history['Val_PSNR'], label='Validation PSNR', color='purple')\n",
    "    plt.title(\"Signal Quality (PSNR - Higher is Better)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # Plot 4: LPIPS (Lower is better)\n",
    "    plt.subplot(1, 4, 4) # 1 row, 4 columns, index 4\n",
    "    plt.plot(history['Val_LPIPS'], label='Validation LPIPS', color='red')\n",
    "    plt.title(\"Perceptual (LPIPS - Lower is Better)\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TwhgwagwY81s"
   },
   "source": [
    "## **Main Training Loop**\n",
    "\n",
    "This is the core execution block. It runs the training over the specified number of epochs using a Two-Time-Scale Update Rule (TTUR) (Discriminator and Generator are updated separately).\n",
    "\n",
    "###**Loss Function Strategy:**\n",
    "\n",
    "- **L1 Loss (Pixel-level):** Penalizes the absolute distance between the predicted color and real color. This ensures general color accuracy but can lead to desaturated (\"sepia\") results.\n",
    "- **Adversarial Loss (GAN):** The Discriminator tries to distinguish real images from generated ones. This forces the Generator to create vibrant, realistic textures to \"fool\" the Discriminator.\n",
    "- **Perceptual Loss (LPIPS):** Uses a pre-trained VGG network to compare high-level features (texture, structure) rather than just pixel values. This aligns the result with human perception.\n",
    "\n",
    "### **Training Phases:**\n",
    "\n",
    "- **Warmup:** The model trains only with L1 loss for the first 20 epochs to stabilize the weights.\n",
    "- **GAN Training:** The model switches to full adversarial training.\n",
    "- **Validation:** At the end of every epoch, the model is evaluated on unseen data, and the weights with the best L1 score are saved.\n",
    "\n",
    "**References:**\n",
    "\n",
    "- **TTUR:** Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., & Hochreiter, S. (2017). GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium. NIPS.\n",
    "- **Perceptual Loss:** Zhang, R., Isola, P., Efros, A. A., Shechtman, E., & Wang, O. (2018). The Unreasonable Effectiveness of Deep Features as a Perceptual Metric. CVPR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZQmzWUPok5M"
   },
   "outputs": [],
   "source": [
    "# --- 1. Setup Optimizers & Loss ---\n",
    "# We use BCEWithLogitsLoss for stability in GANs\n",
    "criterion_GAN = nn.BCEWithLogitsLoss()\n",
    "criterion_L1 = nn.L1Loss()\n",
    "\n",
    "# Initialize Perceptual Loss (LPIPS)\n",
    "# We use VGG as the backbone because it aligns best with human perception\n",
    "print(\"Loading LPIPS VGG model...\")\n",
    "criterion_percep = lpips.LPIPS(net='vgg').to(device)\n",
    "\n",
    "# Lower learning rate slightly for stability\n",
    "lr = 1e-4\n",
    "opt_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "opt_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Use CosineAnnealing for smoother decay\n",
    "num_epochs = 100\n",
    "\n",
    "# Initialize schedulers\n",
    "scheduler_G = optim.lr_scheduler.CosineAnnealingLR(opt_G, T_max=num_epochs, eta_min=1e-6)\n",
    "scheduler_D = optim.lr_scheduler.CosineAnnealingLR(opt_D, T_max=num_epochs, eta_min=1e-6)\n",
    "\n",
    "# Weights\n",
    "lambda_L1 = 50.0\n",
    "lambda_percep = 10.0\n",
    "\n",
    "# Tracking\n",
    "best_val_loss = float('inf')\n",
    "history = {'G_loss': [], 'D_loss': [], 'Val_L1': [], 'Val_PSNR': [], 'Val_LPIPS': []}\n",
    "\n",
    "print(\"STARTING....\")\n",
    "print(f\"Device: {device} | Epochs: {num_epochs}\")\n",
    "\n",
    "def validate_model(gen, loader, device):\n",
    "    \"\"\"Calculates L1, PSNR, and LPIPS on validation set\"\"\"\n",
    "    gen.eval()\n",
    "    val_loss = 0.0\n",
    "    val_psnr = 0.0\n",
    "    val_lpips = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            L = batch['L'].to(device)\n",
    "            ab = batch['ab'].to(device)\n",
    "            fake_ab = gen(L)\n",
    "\n",
    "            # L1 Loss\n",
    "            val_loss += criterion_L1(fake_ab, ab).item()\n",
    "\n",
    "            # PSNR Calculation\n",
    "            mse = torch.mean((ab - fake_ab) ** 2)\n",
    "            if mse == 0:\n",
    "                val_psnr += 100\n",
    "            else:\n",
    "                psnr = 10 * torch.log10(4.0 / mse)\n",
    "                val_psnr += psnr.item()\n",
    "\n",
    "            # LPIPS Calculation\n",
    "            # LPIPS expects 3 channels. We concatenate L and ab.\n",
    "            real_stack = torch.cat([L, ab], dim=1)\n",
    "            fake_stack = torch.cat([L, fake_ab], dim=1)\n",
    "\n",
    "            # lpips returns a tensor, we need the float item\n",
    "            batch_lpips = criterion_percep(fake_stack, real_stack)\n",
    "            val_lpips += batch_lpips.mean().item()\n",
    "\n",
    "    gen.train()\n",
    "    # Return average L1, PSNR, and LPIPS\n",
    "    return val_loss / len(loader), val_psnr / len(loader), val_lpips / len(loader)\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    generator.train()\n",
    "    discriminator.train()\n",
    "\n",
    "    g_loss_epoch = 0.0\n",
    "    d_loss_epoch = 0.0\n",
    "\n",
    "    # Warmup: Train only Generator with L1 loss for first 10 epochs\n",
    "    warmup = epoch <= 20\n",
    "\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "\n",
    "    for batch in loop:\n",
    "        L = batch['L'].to(device)\n",
    "        ab_real = batch['ab'].to(device)\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "        loss_D_val = 0.0\n",
    "\n",
    "        if not warmup:\n",
    "            opt_D.zero_grad()\n",
    "\n",
    "            # Generate fake image\n",
    "            fake_ab = generator(L)\n",
    "\n",
    "            # Concatenate L + ab\n",
    "            real_image = torch.cat([L, ab_real], dim=1)\n",
    "            fake_image = torch.cat([L, fake_ab.detach()], dim=1)\n",
    "\n",
    "            # Add slight noise to inputs to stabilize Discriminator\n",
    "            noise = torch.randn_like(real_image) * 0.05\n",
    "\n",
    "            # Discriminator forward pass\n",
    "            pred_real = discriminator(real_image + noise)\n",
    "            pred_fake = discriminator(fake_image + noise)\n",
    "\n",
    "            # Label Smoothing\n",
    "            valid = torch.ones_like(pred_real) * 0.9\n",
    "            fake = torch.zeros_like(pred_fake) + 0.1\n",
    "\n",
    "            loss_D_real = criterion_GAN(pred_real, valid)\n",
    "            loss_D_fake = criterion_GAN(pred_fake, fake)\n",
    "            loss_D = (loss_D_real + loss_D_fake) * 0.5\n",
    "\n",
    "            loss_D.backward()\n",
    "            loss_D_val = loss_D.item()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(discriminator.parameters(), max_norm=1.0)\n",
    "            opt_D.step()\n",
    "\n",
    "        # -----------------\n",
    "        #  Train Generator\n",
    "        # -----------------\n",
    "        opt_G.zero_grad()\n",
    "\n",
    "        # Re-generate fake_ab for Generator update\n",
    "        fake_ab = generator(L)\n",
    "        fake_image = torch.cat([L, fake_ab], dim=1) # Used for GAN loss\n",
    "\n",
    "        # We also need the real image stack for Perceptual Loss\n",
    "        real_image_stack = torch.cat([L, ab_real], dim=1)\n",
    "\n",
    "        # 1. Pixel-level Loss (L1)\n",
    "        loss_G_L1 = criterion_L1(fake_ab, ab_real) * lambda_L1\n",
    "\n",
    "        # 2. GAN Loss (only after warmup)\n",
    "        loss_G_GAN = 0.0\n",
    "        if not warmup:\n",
    "            pred_fake = discriminator(fake_image)\n",
    "            loss_G_GAN = criterion_GAN(pred_fake, torch.ones_like(pred_fake))\n",
    "\n",
    "        # 3. Perceptual Loss (LPIPS)\n",
    "        loss_G_percep = 0.0\n",
    "        if lambda_percep > 0:\n",
    "            loss_G_percep = criterion_percep(fake_image, real_image_stack).mean() * lambda_percep\n",
    "\n",
    "        # Total Generator Loss\n",
    "        loss_G = loss_G_L1 + loss_G_GAN + loss_G_percep\n",
    "\n",
    "        loss_G.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(generator.parameters(), max_norm=1.0)\n",
    "\n",
    "        opt_G.step()\n",
    "\n",
    "        # Logging\n",
    "        g_loss_epoch += loss_G.item()\n",
    "        d_loss_epoch += loss_D_val\n",
    "\n",
    "        status = \"Warmup\" if warmup else \"GAN\"\n",
    "        loop.set_description(f\"Epoch [{epoch}/{num_epochs}] ({status})\")\n",
    "        loop.set_postfix(L1=loss_G_L1.item(), Percep=loss_G_percep.item() if not isinstance(loss_G_percep, float) else 0, D_loss=loss_D_val)\n",
    "\n",
    "    # --- End of Epoch Updates ---\n",
    "    scheduler_G.step()\n",
    "    scheduler_D.step()\n",
    "\n",
    "    avg_g_loss = g_loss_epoch / len(train_loader)\n",
    "    avg_d_loss = d_loss_epoch / len(train_loader)\n",
    "\n",
    "    # Validate on unseen data\n",
    "    current_val_loss, current_psnr, current_lpips = validate_model(generator, val_loader, device)\n",
    "\n",
    "    # Store history\n",
    "    history['G_loss'].append(avg_g_loss)\n",
    "    history['D_loss'].append(avg_d_loss)\n",
    "    history['Val_L1'].append(current_val_loss)\n",
    "    history['Val_PSNR'].append(current_psnr)\n",
    "    history['Val_LPIPS'].append(current_lpips)\n",
    "\n",
    "    # --- CRITICAL FIX: Save Best Model based on VALIDATION L1 ---\n",
    "    # We ONLY save if epoch > 20.\n",
    "    # This ignores the \"fake best\" results from the warmup phase (epochs 1-10)\n",
    "    # where the model just outputs gray to cheat the L1 loss.\n",
    "    if current_val_loss < best_val_loss and epoch > 20:\n",
    "        best_val_loss = current_val_loss\n",
    "        torch.save(generator.state_dict(), \"colorizer_BEST.pth\")\n",
    "        best_msg = f\" (New Best L1: {best_val_loss:.4f})\"\n",
    "    else:\n",
    "        best_msg = \"\"\n",
    "\n",
    "    # Periodic Save\n",
    "    if epoch % 10 == 0:\n",
    "        torch.save(generator.state_dict(), f\"colorizer_epoch_{epoch}.pth\")\n",
    "\n",
    "    # Visualization\n",
    "    clear_output(wait=True)\n",
    "    print(f\"EPOCH {epoch}/{num_epochs} | LR: {opt_G.param_groups[0]['lr']:.6f}\")\n",
    "    print(f\"Train G Loss: {avg_g_loss:.4f} | Train D Loss: {avg_d_loss:.4f}\")\n",
    "    print(f\"Val L1: {current_val_loss:.4f} | Val PSNR: {current_psnr:.2f} dB | Val LPIPS: {current_lpips:.4f} {best_msg}\")\n",
    "\n",
    "    # Call the helper function for plotting\n",
    "    plot_training_history(history)\n",
    "\n",
    "    # Show visual examples every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        try: show_results_resnet(generator, test_loader, num_images=3)\n",
    "        except: pass\n",
    "\n",
    "# Save final\n",
    "torch.save(generator.state_dict(), \"colorizer_FINAL.pth\")\n",
    "print(\"TRAINING COMPLETE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGPMhvLyp8ur"
   },
   "source": [
    "----\n",
    "#**Evaluation Procedures üìà**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AofU3HC1qaB0"
   },
   "source": [
    "## **Training Diagnostics**\n",
    "\n",
    "Visualizing training progress is essential for debugging GANs. This cell generates smoothed plots for:\n",
    "\n",
    "- **Adversarial Dynamics:** Comparing Generator Loss vs. Discriminator Loss to ensure neither is overpowering the other.\n",
    "- **Validation Accuracy:** Tracking the L1 loss on the validation set to detect overfitting (when validation loss starts rising while training loss continues to fall).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 684
    },
    "id": "PjVSYHxw0Z10",
    "outputId": "efae5213-f897-472c-b92d-48426df32cbc"
   },
   "outputs": [],
   "source": [
    "# ==================== CELL 8: ADVANCED TRAINING DIAGNOSTICS (ROBUST HISTORY KEYS) ====================\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import numpy as np\n",
    "\n",
    "# --- CONFIGURACI√ìN ---\n",
    "GAN_START_EPOCH = 20\n",
    "SMOOTHING_FACTOR = 0.85  # 0.0‚Äì1.0 (mayor = m√°s suave)\n",
    "\n",
    "# Paleta profesional\n",
    "COLOR_G_RAW = '#A9CCE3'     # Azul claro\n",
    "COLOR_G_SMOOTH = '#2874A6'  # Azul\n",
    "COLOR_D_RAW = '#F5B7B1'     # Rojo claro\n",
    "COLOR_D_SMOOTH = '#CB4335'  # Rojo\n",
    "COLOR_VAL = '#229954'       # Verde\n",
    "COLOR_WARMUP = '#EAEDED'    # Gris claro\n",
    "\n",
    "plt.rcParams[\"figure.dpi\"] = 160\n",
    "\n",
    "print(\"RUNNING ADVANCED DIAGNOSTICS...\")\n",
    "\n",
    "def get_first_key(dct, candidates, default=None):\n",
    "    \"\"\"Devuelve el primer key existente en dct de la lista candidates; si no, default.\"\"\"\n",
    "    for k in candidates:\n",
    "        if isinstance(dct, dict) and k in dct and dct[k] is not None:\n",
    "            return k\n",
    "    return default\n",
    "\n",
    "# --- 1) Recuperar history de forma robusta ---\n",
    "need_dummy = False\n",
    "if 'history' not in globals() or not isinstance(history, dict) or len(history) == 0:\n",
    "    need_dummy = True\n",
    "\n",
    "if need_dummy:\n",
    "    print(\"‚ö†Ô∏è No training history found. Generating dummy data for demo...\")\n",
    "    epochs_range = 80\n",
    "    history = {\n",
    "        'G_loss': [0.5 * (0.92**i) for i in range(20)] + [2.5 + (0.98**(i-20)) + np.random.normal(0, 0.1) for i in range(20, epochs_range)],\n",
    "        'D_loss': [0.1 for _ in range(20)] + [0.6 + (0.99**(i-20)) + np.random.normal(0, 0.05) for i in range(20, epochs_range)],\n",
    "        'Val_L1': [0.11 - 0.0006*i + np.random.normal(0, 0.0015) for i in range(epochs_range)]\n",
    "    }\n",
    "\n",
    "# Intentar mapear claves t√≠picas\n",
    "g_key = get_first_key(history, ['G', 'G_loss', 'loss_G'])\n",
    "d_key = get_first_key(history, ['D', 'D_loss', 'loss_D'])\n",
    "val_l1_key = get_first_key(history, ['Val_L1', 'L1_val', 'val_l1'])\n",
    "\n",
    "# Validaci√≥n opcional adicional (si existe)\n",
    "val_psnr_key = get_first_key(history, ['Val_PSNR', 'val_psnr'])\n",
    "val_lpips_key = get_first_key(history, ['Val_LPIPS', 'val_lpips'])\n",
    "\n",
    "if g_key is None or d_key is None:\n",
    "    # Si aun as√≠ faltan G/D, generamos dummy con longitudes consistentes\n",
    "    print(\"‚ÑπÔ∏è History keys missing for G/D. Synthesizing demo curves...\")\n",
    "    epochs_range = len(history[val_l1_key]) if val_l1_key else 80\n",
    "    history[g_key or 'G_loss'] = [0.5 * (0.92**i) for i in range(min(20, epochs_range))] + \\\n",
    "                                 [2.2 + (0.98**max(0, i-20)) + np.random.normal(0, 0.08) for i in range(20, epochs_range)]\n",
    "    history[d_key or 'D_loss'] = [0.1 for _ in range(min(20, epochs_range))] + \\\n",
    "                                 [0.6 + (0.99**max(0, i-20)) + np.random.normal(0, 0.05) for i in range(20, epochs_range)]\n",
    "    if g_key is None: g_key = 'G_loss'\n",
    "    if d_key is None: d_key = 'D_loss'\n",
    "\n",
    "# Extraer arrays\n",
    "G_losses = np.array(history[g_key], dtype=float)\n",
    "D_losses = np.array(history[d_key], dtype=float)\n",
    "epochs = np.arange(1, len(G_losses) + 1)\n",
    "\n",
    "# Alinear longitudes si D tiene otra longitud (poco com√∫n)\n",
    "min_len = min(len(G_losses), len(D_losses))\n",
    "if min_len < len(G_losses): G_losses = G_losses[:min_len]\n",
    "if min_len < len(D_losses): D_losses = D_losses[:min_len]\n",
    "epochs = np.arange(1, min_len + 1)\n",
    "\n",
    "# --- 2) Suavizado EMA ---\n",
    "def smooth_curve(scalars, weight):\n",
    "    if len(scalars) == 0: return np.array([])\n",
    "    last = scalars[0]\n",
    "    smoothed = []\n",
    "    for point in scalars:\n",
    "        smoothed_val = last * weight + (1 - weight) * point\n",
    "        smoothed.append(smoothed_val)\n",
    "        last = smoothed_val\n",
    "    return np.array(smoothed)\n",
    "\n",
    "G_smooth = smooth_curve(G_losses, SMOOTHING_FACTOR)\n",
    "D_smooth = smooth_curve(D_losses, SMOOTHING_FACTOR)\n",
    "\n",
    "# Val L1 si est√° disponible\n",
    "Val_losses = None\n",
    "Val_smooth = None\n",
    "val_epochs = None\n",
    "if val_l1_key is not None and isinstance(history[val_l1_key], (list, tuple)):\n",
    "    Val_losses = np.array(history[val_l1_key], dtype=float)\n",
    "    # Reamostrar ejes si longitudes difieren\n",
    "    if len(Val_losses) != len(G_losses):\n",
    "        val_epochs = np.linspace(1, len(G_losses), len(Val_losses))\n",
    "    else:\n",
    "        val_epochs = epochs.copy()\n",
    "    Val_smooth = smooth_curve(Val_losses, 0.85)\n",
    "\n",
    "# --- 3) Graficado ---\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "fig, axes = plt.subplots(1, 2, figsize=(22, 8))\n",
    "ax1, ax2 = axes\n",
    "fig.suptitle('Training Performance Diagnostics', fontsize=22, fontweight='bold', y=0.98)\n",
    "\n",
    "# Subplot 1: Din√°mica adversaria\n",
    "ax1.plot(epochs, G_losses, color=COLOR_G_RAW, alpha=0.35, linewidth=1, label='_nolegend_')\n",
    "ax1.plot(epochs, D_losses, color=COLOR_D_RAW, alpha=0.35, linewidth=1, label='_nolegend_')\n",
    "ax1.plot(epochs, G_smooth, color=COLOR_G_SMOOTH, linewidth=2.8, label='Generator Loss')\n",
    "ax1.plot(epochs, D_smooth, color=COLOR_D_SMOOTH, linewidth=2.8, label='Discriminator Loss')\n",
    "\n",
    "if len(epochs) > GAN_START_EPOCH:\n",
    "    ylim = ax1.get_ylim()\n",
    "    height = (ylim[1] - ylim[0])\n",
    "    rect = patches.Rectangle((0, ylim[0]), GAN_START_EPOCH, height,\n",
    "                             linewidth=0, edgecolor='none', facecolor=COLOR_WARMUP, alpha=0.6, zorder=0)\n",
    "    ax1.add_patch(rect)\n",
    "    ax1.axvline(x=GAN_START_EPOCH, color='#7F8C8D', linestyle='--', linewidth=1.6)\n",
    "    ax1.text(GAN_START_EPOCH/2, ylim[0] + height*0.94, \"WARMUP PHASE\", ha='center', fontsize=11, fontweight='bold', color='#7F8C8D')\n",
    "    ax1.text(GAN_START_EPOCH + (len(epochs)-GAN_START_EPOCH)/2, ylim[0] + height*0.94, \"GAN TRAINING PHASE\", ha='center', fontsize=11, fontweight='bold', color='#2C3E50')\n",
    "\n",
    "ax1.set_title('Global Loss Landscape', fontsize=16, pad=10)\n",
    "ax1.set_xlabel('Epoch', fontsize=13)\n",
    "ax1.set_ylabel('Loss Value', fontsize=13)\n",
    "ax1.legend(loc='upper right', frameon=True, framealpha=1, shadow=True)\n",
    "ax1.grid(True, linestyle=':', alpha=0.6)\n",
    "\n",
    "# Subplot 2: Estabilidad y mejor √©poca (usa Val_L1 si existe; si no, usa G_smooth)\n",
    "if len(epochs) > GAN_START_EPOCH + 2:\n",
    "    if Val_smooth is not None and len(Val_smooth) > 0:\n",
    "        # Eje de validaci√≥n\n",
    "        ax2.plot(val_epochs, Val_losses, color=COLOR_VAL, alpha=0.25, label='Raw Val L1')\n",
    "        ax2.plot(val_epochs, Val_smooth, color=COLOR_VAL, linewidth=2.8, label='Smoothed Val L1')\n",
    "\n",
    "        valid_mask = (val_epochs > GAN_START_EPOCH)\n",
    "        if np.any(valid_mask):\n",
    "            gan_phase_epochs = val_epochs[valid_mask]\n",
    "            gan_phase_vals = Val_smooth[valid_mask]\n",
    "            local_min_idx = int(np.argmin(gan_phase_vals))\n",
    "            min_epoch = float(gan_phase_epochs[local_min_idx])\n",
    "            min_val = float(gan_phase_vals[local_min_idx])\n",
    "        else:\n",
    "            min_idx = int(np.argmin(Val_smooth))\n",
    "            min_epoch = float(val_epochs[min_idx])\n",
    "            min_val = float(Val_smooth[min_idx])\n",
    "\n",
    "        ax2.scatter(min_epoch, min_val, color='#E74C3C', s=110, zorder=5, edgecolors='white', linewidth=2)\n",
    "        if len(epochs) > GAN_START_EPOCH:\n",
    "            ax2.axvspan(0, GAN_START_EPOCH, color=COLOR_WARMUP, alpha=0.5, zorder=0)\n",
    "            ax2.axvline(x=GAN_START_EPOCH, color='#7F8C8D', linestyle='--', linewidth=1.6)\n",
    "\n",
    "        y_range = float(np.max(Val_smooth) - np.min(Val_smooth))\n",
    "        y_text = min_val + (y_range * 0.12 if y_range > 0 else 0.01)\n",
    "        ax2.annotate(f'Best (Post-Warmup)\\nEpoch {int(round(min_epoch))}\\nL1: {min_val:.4f}',\n",
    "                     xy=(min_epoch, min_val),\n",
    "                     xytext=(min_epoch, y_text),\n",
    "                     arrowprops=dict(facecolor='black', shrink=0.05, width=1, headwidth=8),\n",
    "                     bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.9),\n",
    "                     ha='center', fontsize=11)\n",
    "\n",
    "        ax2.set_title('Color Accuracy (Validation L1 Loss)', fontsize=16, pad=10)\n",
    "        ax2.set_xlabel('Epoch', fontsize=13)\n",
    "        ax2.set_ylabel('L1 Loss (Lower is Better)', fontsize=13)\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, linestyle=':', alpha=0.6)\n",
    "\n",
    "        # Reporte textual basado en Val_L1\n",
    "        print(f\"\\n{'='*25}  TRAINING REPORT  {'='*25}\")\n",
    "        current_val = float(Val_smooth[-1])\n",
    "        delta = current_val - min_val\n",
    "        print(f\"üîπ Best (Post-Warmup):   Epoch {int(round(min_epoch))} (L1: {min_val:.4f})\")\n",
    "        print(f\"üîπ Current Status:       Epoch {int(round(val_epochs[-1]))} (L1: {current_val:.4f})\")\n",
    "        print(\"-\" * 67)\n",
    "        if delta > 0.015:\n",
    "            print(f\"‚ö†Ô∏è  OVERFITTING DETECTED: Validation loss increased by +{delta:.4f} vs. best.\")\n",
    "        elif 0 < delta <= 0.015:\n",
    "            print(f\"‚ÑπÔ∏è  STABLE: Slight fluctuation (+{delta:.4f}) is normal for GANs.\")\n",
    "        else:\n",
    "            print(\"‚úÖ EXCELLENT: Current model matches or surpasses the best observed.\")\n",
    "    else:\n",
    "        # Si no hay Val_L1, mostramos estabilidad de G_smooth\n",
    "        gan_epochs = epochs[GAN_START_EPOCH:]\n",
    "        gan_G_smooth = G_smooth[GAN_START_EPOCH:]\n",
    "        ax2.plot(gan_epochs, gan_G_smooth, color=COLOR_G_SMOOTH, linewidth=2.6, label='G (Smoothed)')\n",
    "        min_idx = int(np.argmin(gan_G_smooth))\n",
    "        min_epoch = int(gan_epochs[min_idx])\n",
    "        min_val = float(gan_G_smooth[min_idx])\n",
    "        ax2.scatter(min_epoch, min_val, color='#E74C3C', s=110, zorder=5, edgecolors='white', linewidth=2)\n",
    "        ax2.set_title('Generator Stability Analysis (Post-Warmup)', fontsize=16, pad=10)\n",
    "        ax2.set_xlabel('Epoch', fontsize=13)\n",
    "        ax2.set_ylabel('Smoothed Generator Loss', fontsize=13)\n",
    "        ax2.grid(True, linestyle=':', alpha=0.6)\n",
    "        ax2.legend()\n",
    "\n",
    "        print(f\"\\n{'='*25}  TRAINING REPORT  {'='*25}\")\n",
    "        current_loss = float(G_smooth[-1])\n",
    "        lowest_loss = float(np.min(gan_G_smooth))\n",
    "        delta = current_loss - lowest_loss\n",
    "        print(f\"üîπ Best Performance:   Epoch {min_epoch} (G Loss: {lowest_loss:.4f})\")\n",
    "        print(f\"üîπ Current Status:     Epoch {int(epochs[-1])} (G Loss: {current_loss:.4f})\")\n",
    "        print(\"-\" * 67)\n",
    "        if delta > 0.30:\n",
    "            print(\"‚ö†Ô∏è  CRITICAL: MODEL DIVERGENCE DETECTED\")\n",
    "            print(f\"   The model loss has spiked (+{delta:.2f}) from its best point.\")\n",
    "        elif delta > 0.05:\n",
    "            print(\"üî∏ NOTICE: Slight fluctuation.\")\n",
    "            print(\"   The model is slightly worse than its peak. This is normal in GANs.\")\n",
    "        else:\n",
    "            print(\"‚úÖ STABLE: The model is currently performing near its peak ability.\")\n",
    "else:\n",
    "    ax2.text(0.5, 0.5, \"Insufficient data for Stability Analysis\\n(Wait for GAN Phase)\",\n",
    "             ha='center', va='center', fontsize=14, color='gray')\n",
    "    ax2.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMsSq-14qjDG"
   },
   "source": [
    "## **Quantitative Evaluation (PSNR & SSIM)**\n",
    "\n",
    "This cell loads the best-performing model weights and runs a rigorous evaluation on the test set. It calculates two industry-standard metrics:\n",
    "\n",
    "- **PSNR** (Peak Signal-to-Noise Ratio): Measures the quality of image reconstruction.\n",
    "- **SSIM** (Structural Similarity Index): Measures the structural similarity between the generated image and the ground truth.\n",
    "\n",
    "The results are sorted to identify the best and worst performing cases.\n",
    "\n",
    "**References:**\n",
    "\n",
    "- **SSIM:** Wang, Z., Bovik, A. C., Sheikh, H. R., & Simoncelli, E. P. (2004). Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zfujTAVkqk2q"
   },
   "outputs": [],
   "source": [
    "# ==================== CELL 9: DYNAMIC MODEL EVALUATION + GALLERY (IMPROVED VISUALS) ====================\n",
    "import os\n",
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import lab2rgb\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "# Enhanced styling\n",
    "plt.style.use('default')  # Start clean\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 160,\n",
    "    \"figure.facecolor\": \"white\",\n",
    "    \"axes.titlesize\": 16,\n",
    "    \"axes.labelsize\": 14,\n",
    "    \"axes.titleweight\": \"bold\",\n",
    "    \"axes.grid\": False,\n",
    "    \"legend.fontsize\": 12,\n",
    "    \"font.family\": \"sans-serif\",\n",
    "    \"text.usetex\": False,\n",
    "    \"savefig.bbox\": \"tight\",\n",
    "    \"savefig.pad_inches\": 0.2\n",
    "})\n",
    "\n",
    "SECTION_TITLE_SIZE = 24\n",
    "SUBTITLE_SIZE = 18\n",
    "CAPTION_SIZE = 14\n",
    "METRIC_FONTSIZE = 13\n",
    "\n",
    "SAVE_FIGS = False\n",
    "SAVE_PREFIX = \"eval_gallery\"\n",
    "\n",
    "# ---- (Your load_best_available_weights and lab_batch_to_rgb functions remain unchanged) ----\n",
    "\n",
    "# (Assume generator, device, test_loader are already defined)\n",
    "\n",
    "# Load weights\n",
    "which, loaded = load_best_available_weights(generator, device)\n",
    "print(f\"--> Loaded checkpoint for evaluation: {which if loaded else 'in-memory'}\")\n",
    "generator.eval()\n",
    "\n",
    "# Evaluate (unchanged logic)\n",
    "NUM_SAMPLES_TO_TEST = 100\n",
    "AB_SCALE_FOR_EVAL = 128.0\n",
    "results = []\n",
    "print(f\"Running evaluation on {NUM_SAMPLES_TO_TEST} test images...\")\n",
    "with torch.no_grad():\n",
    "    counted = 0\n",
    "    for batch in test_loader:\n",
    "        if counted >= NUM_SAMPLES_TO_TEST:\n",
    "            break\n",
    "        L = batch['L'].to(device)\n",
    "        ab_real = batch['ab'].to(device)\n",
    "        ab_fake = generator(L)\n",
    "        fake_rgb = lab_batch_to_rgb(L, ab_fake, AB_SCALE_FOR_EVAL)\n",
    "        real_rgb = lab_batch_to_rgb(L, ab_real, AB_SCALE_FOR_EVAL)\n",
    "        gray_disp = ((L.detach().cpu().numpy().squeeze(1)) + 1.0) / 2.0\n",
    "        for i in range(fake_rgb.shape[0]):\n",
    "            if counted >= NUM_SAMPLES_TO_TEST: break\n",
    "            p = psnr(real_rgb[i], fake_rgb[i], data_range=1.0)\n",
    "            s = ssim(real_rgb[i], fake_rgb[i], channel_axis=2, data_range=1.0)\n",
    "            results.append({\n",
    "                'gray': gray_disp[i],\n",
    "                'real': real_rgb[i],\n",
    "                'fake': fake_rgb[i],\n",
    "                'psnr': float(p),\n",
    "                'ssim': float(s)\n",
    "            })\n",
    "            counted += 1\n",
    "\n",
    "if len(results) == 0:\n",
    "    print(\"No results gathered. Check your test_loader.\")\n",
    "else:\n",
    "    avg_psnr = np.mean([r['psnr'] for r in results])\n",
    "    avg_ssim = np.mean([r['ssim'] for r in results])\n",
    "\n",
    "    # ==================== 1. Overview Summary ====================\n",
    "    fig = plt.figure(figsize=(20, 3.5))\n",
    "    fig.suptitle(\"Model Evaluation Overview\", fontsize=SECTION_TITLE_SIZE, fontweight='bold', y=0.98)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axis('off')\n",
    "\n",
    "    summary_text = (\n",
    "        f\"Checkpoint: {which if loaded else 'in-memory'}\\n\"\n",
    "        f\"Test Samples: {counted}‚ÄÉ‚ÄÉAB Scale: {AB_SCALE_FOR_EVAL:.0f}\\n\\n\"\n",
    "        f\"Average SSIM:  {avg_ssim:.4f}‚ÄÉ‚ÄÉ\"\n",
    "        f\"Average PSNR:  {avg_psnr:.2f} dB\"\n",
    "    )\n",
    "    ax.text(0.02, 0.5, summary_text, fontsize=18, va='center', linespacing=1.6,\n",
    "            bbox=dict(boxstyle=\"round,pad=1\", facecolor=\"#f8f9fa\", edgecolor=\"none\"))\n",
    "\n",
    "    if SAVE_FIGS:\n",
    "        plt.savefig(f\"{SAVE_PREFIX}_overview.png\", dpi=200)\n",
    "    plt.show()\n",
    "\n",
    "    # ==================== 2. Gallery Function (Reusable) ====================\n",
    "    def plot_gallery(title, samples, nrows=8, sort_by='ssim', ascending=False):\n",
    "        samples = sorted(samples, key=lambda x: x[sort_by], reverse=not ascending)\n",
    "        samples = samples[:nrows]\n",
    "\n",
    "        fig = plt.figure(figsize=(24, 5.2 * nrows))\n",
    "        fig.suptitle(title, fontsize=SECTION_TITLE_SIZE, fontweight='bold', y=0.99)\n",
    "\n",
    "        for idx, r in enumerate(samples):\n",
    "            # Input (Grayscale)\n",
    "            ax1 = plt.subplot(nrows, 3, 3*idx + 1)\n",
    "            ax1.imshow(r['gray'], cmap='gray', vmin=0, vmax=1)\n",
    "            ax1.set_title(\"Input (Grayscale)\", fontsize=SUBTITLE_SIZE, pad=12)\n",
    "            ax1.axis('off')\n",
    "\n",
    "            # Model Output\n",
    "            ax2 = plt.subplot(nrows, 3, 3*idx + 2)\n",
    "            ax2.imshow(r['fake'])\n",
    "            ax2.set_title(\"Model Output\", fontsize=SUBTITLE_SIZE, pad=12)\n",
    "            # Overlay metrics\n",
    "            ax2.text(0.98, 0.02, f\"SSIM: {r['ssim']:.3f}\\nPSNR: {r['psnr']:.1f} dB\",\n",
    "                     transform=ax2.transAxes, fontsize=METRIC_FONTSIZE,\n",
    "                     ha='right', va='bottom', color='white',\n",
    "                     bbox=dict(boxstyle=\"round,pad=0.4\", facecolor='black', alpha=0.7))\n",
    "            ax2.axis('off')\n",
    "\n",
    "            # Ground Truth\n",
    "            ax3 = plt.subplot(nrows, 3, 3*idx + 3)\n",
    "            ax3.imshow(r['real'])\n",
    "            ax3.set_title(\"Ground Truth\", fontsize=SUBTITLE_SIZE, pad=12)\n",
    "            ax3.axis('off')\n",
    "\n",
    "        # Shared caption\n",
    "        caption = (\"Left: Input grayscale (L channel)‚ÄÉ‚ÄÉ\"\n",
    "                   \"Middle: Model colorization‚ÄÉ‚ÄÉ\"\n",
    "                   \"Right: Original color image\\n\"\n",
    "                   f\"Sorted by SSIM ({'descending' if not ascending else 'ascending'}). \"\n",
    "                   f\"Metrics shown on model output.\")\n",
    "        fig.text(0.5, 0.01, caption, ha='center', fontsize=CAPTION_SIZE, style='italic')\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.97])\n",
    "        if SAVE_FIGS:\n",
    "            suffix = \"best\" if not ascending else \"worst\"\n",
    "            plt.savefig(f\"{SAVE_PREFIX}_{suffix}.png\", dpi=200)\n",
    "        plt.show()\n",
    "\n",
    "    # ==================== 3. Best Cases ====================\n",
    "    plot_gallery(\"Best-Performing Cases (Highest SSIM)\", results, nrows=8, sort_by='ssim', ascending=False)\n",
    "\n",
    "    # ==================== 4. Challenging Cases ====================\n",
    "    plot_gallery(\"Challenging / Failure Cases (Lowest SSIM)\", results, nrows=8, sort_by='ssim', ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QWUfRVJAuwgq"
   },
   "source": [
    "#**Test Model Generalization**\n",
    "\n",
    "This final cell allows for real-world testing. Upload any custom black-and-white image, and the model will preprocess it, run the inference using the best saved weights, and display the colorized result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 848
    },
    "id": "uTLcRYCQugnP",
    "outputId": "70055b53-09ee-4c25-b742-173c23bdf670"
   },
   "outputs": [],
   "source": [
    "# ==================== CELL: COLORIZATION INFERENCE (TRAIN-STYLE PREPROCESS + SAFE PAD + MILD BOOST) ====================\n",
    "from google.colab import files\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import io, torch, cv2, os, re\n",
    "from skimage import color\n",
    "\n",
    "# ---- Unified Best-Available Loader (re-use if already defined) ----\n",
    "def load_best_available_weights(generator, device):\n",
    "    def try_load(path):\n",
    "        ckpt = torch.load(path, map_location=device)\n",
    "        state = ckpt['model_state_dict'] if isinstance(ckpt, dict) and 'model_state_dict' in ckpt else ckpt\n",
    "        generator.load_state_dict(state)\n",
    "        return True\n",
    "    if os.path.exists(\"colorizer_GANBEST.pth\"):\n",
    "        try: try_load(\"colorizer_GANBEST.pth\"); return \"colorizer_GANBEST.pth\", True\n",
    "        except Exception as e: print(f\"Note: GANBEST load failed: {e}\")\n",
    "    if os.path.exists(\"colorizer_FINAL.pth\"):\n",
    "        try: try_load(\"colorizer_FINAL.pth\"); return \"colorizer_FINAL.pth\", True\n",
    "        except Exception as e: print(f\"Note: FINAL load failed: {e}\")\n",
    "    epoch_files = [f for f in os.listdir('.') if re.match(r'^colorizer_epoch_\\d+\\.pth$', f)]\n",
    "    if epoch_files:\n",
    "        epoch_files.sort(key=lambda n: int(re.findall(r'\\d+', n)[-1]), reverse=True)\n",
    "        for f in epoch_files:\n",
    "            try: try_load(f); return f, True\n",
    "            except Exception as e: print(f\"Note: load failed for {f}: {e}\")\n",
    "    if os.path.exists(\"colorizer_BEST.pth\"):\n",
    "        try: try_load(\"colorizer_BEST.pth\"); return \"colorizer_BEST.pth\", True\n",
    "        except Exception as e: print(f\"Note: BEST load failed: {e}\")\n",
    "    return None, False\n",
    "\n",
    "which, loaded = load_best_available_weights(generator, device)\n",
    "print(f\"Loaded model for inference: {which if loaded else 'in-memory'}\")\n",
    "generator.eval()\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "TARGET_SIZE = (256, 256)  # match training canvas\n",
    "MODEL_STRIDE = 32         # 5 downsamples ‚Üí 32. Use 64 if your net downsamples 6 times.\n",
    "AB_SCALE = 128.0          # keep 128 here for this model\n",
    "\n",
    "# Optional, subtle enhancements (safe for submission)\n",
    "APPLY_L_CLAHE = True\n",
    "CLAHE_CLIP = 2.0\n",
    "CLAHE_TILES = (8, 8)\n",
    "APPLY_L_GAMMA = True\n",
    "L_GAMMA = 0.95\n",
    "EDGE_AWARE_SMOOTHING = True\n",
    "SMOOTH_SIGMA_SPATIAL = 1.0\n",
    "SMOOTH_SIGMA_LUMA = 10.0\n",
    "GLOBAL_AB_GAIN = 1.10\n",
    "CONF_WEIGHTED_BOOST = 0.10\n",
    "CONF_EPS = 1e-6\n",
    "\n",
    "# --- Preprocessing aligned to validation: Resize shortest side to 256, then CenterCrop(256,256) ---\n",
    "def resize_and_centercrop(img_pil, target_size):\n",
    "    w, h = img_pil.size\n",
    "    scale = 256.0 / min(w, h)\n",
    "    new_w, new_h = int(round(w * scale)), int(round(h * scale))\n",
    "    img_resized = img_pil.resize((new_w, new_h), Image.Resampling.LANCZOS)\n",
    "    left = (new_w - target_size[1]) // 2\n",
    "    top  = (new_h - target_size[0]) // 2\n",
    "    return img_resized.crop((left, top, left + target_size[1], top + target_size[0]))\n",
    "\n",
    "def pil_to_rgb_array_train_style(image_bytes, target_size):\n",
    "    img = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "    img = resize_and_centercrop(img, target_size)\n",
    "    return np.array(img)\n",
    "\n",
    "def rgb_to_L(rgb):\n",
    "    rgb_f = rgb.astype(np.float32) / 255.0 if rgb.dtype != np.float32 and rgb.dtype != np.float64 else rgb\n",
    "    return color.rgb2lab(rgb_f)[..., 0]  # [0,100]\n",
    "\n",
    "def L_to_model_input(L):  # [0,100] -> [-1,1]\n",
    "    return (L / 50.0) - 1.0\n",
    "\n",
    "# --- Safe pad/crop for model forward ---\n",
    "def safe_pad_to_stride(img2d, stride=32, mode='reflect'):\n",
    "    H, W = img2d.shape\n",
    "    H_pad = (stride - H % stride) % stride\n",
    "    W_pad = (stride - W % stride) % stride\n",
    "    t, b = H_pad // 2, H_pad - H_pad // 2\n",
    "    l, r = W_pad // 2, W_pad - W_pad // 2\n",
    "    if H_pad == 0 and W_pad == 0:\n",
    "        return img2d, (0,0,0,0)\n",
    "    return np.pad(img2d, ((t,b),(l,r)), mode=mode), (t,b,l,r)\n",
    "\n",
    "def safe_unpad(ab, pads):\n",
    "    t,b,l,r = pads\n",
    "    return ab if (t|b|l|r)==0 else ab[:, t:ab.shape[1]-b, l:ab.shape[2]-r]\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_net_on_Lnorm_safe(L_norm_2d, stride=MODEL_STRIDE):\n",
    "    L_pad, pads = safe_pad_to_stride(L_norm_2d, stride=stride, mode='reflect')\n",
    "    inp = L_pad.reshape(1, 1, L_pad.shape[0], L_pad.shape[1]).astype(np.float32)\n",
    "    out = generator(torch.from_numpy(inp).to(device))  # (1,2,Hp,Wp) in [-1,1]\n",
    "    ab_pad = out.detach().cpu().numpy()[0]\n",
    "    ab = safe_unpad(ab_pad, pads)  # (2,H,W)\n",
    "    return np.ascontiguousarray(ab.astype(np.float32))\n",
    "\n",
    "# --- Optional refinements ---\n",
    "def edge_aware_smooth_ab(L_base, ab_pred, sigma_spatial=1.0, sigma_luma=10.0):\n",
    "    ab_lab = ab_pred * AB_SCALE\n",
    "    a = ab_lab[0].astype(np.float32)\n",
    "    b = ab_lab[1].astype(np.float32)\n",
    "    d = 5\n",
    "    a_s = cv2.bilateralFilter(a, d=d, sigmaColor=sigma_luma, sigmaSpace=sigma_spatial)\n",
    "    b_s = cv2.bilateralFilter(b, d=d, sigmaColor=sigma_luma, sigmaSpace=sigma_spatial)\n",
    "    return np.stack([a_s, b_s], axis=0) / AB_SCALE\n",
    "\n",
    "def apply_l_adjustments(L):\n",
    "    L_adj = L.copy()\n",
    "    if APPLY_L_CLAHE:\n",
    "        L_u8 = np.clip((L_adj / 100.0) * 255.0, 0, 255).astype(np.uint8)\n",
    "        clahe = cv2.createCLAHE(clipLimit=CLAHE_CLIP, tileGridSize=CLAHE_TILES)\n",
    "        L_u8 = clahe.apply(L_u8)\n",
    "        L_adj = (L_u8.astype(np.float32) / 255.0) * 100.0\n",
    "    if APPLY_L_GAMMA:\n",
    "        L01 = np.clip(L_adj / 100.0, 0.0, 1.0)\n",
    "        L01 = np.power(L01, L_GAMMA)\n",
    "        L_adj = np.clip(L01, 0, 1) * 100.0\n",
    "    return L_adj\n",
    "\n",
    "def apply_ab_color_boost(ab_pred, global_gain=1.10, conf_boost=0.10, eps=1e-6):\n",
    "    a, b = ab_pred[0], ab_pred[1]\n",
    "    mag = np.sqrt(a*a + b*b)\n",
    "    mag_norm = np.clip(mag / (1.0 + eps), 0.0, 1.0)\n",
    "    gain_map = global_gain + conf_boost * mag_norm\n",
    "    a_boost = np.clip(a * gain_map, -1.0, 1.0)\n",
    "    b_boost = np.clip(b * gain_map, -1.0, 1.0)\n",
    "    return np.stack([a_boost, b_boost], axis=0)\n",
    "\n",
    "def lab_from_L_and_ab(L_base, ab_pred):\n",
    "    ab_lab = ab_pred * AB_SCALE\n",
    "    H, W = L_base.shape\n",
    "    lab = np.zeros((H, W, 3), dtype=np.float64)\n",
    "    lab[..., 0] = np.clip(L_base, 0, 100)\n",
    "    lab[..., 1] = np.clip(ab_lab[0], -128, 127)\n",
    "    lab[..., 2] = np.clip(ab_lab[1], -128, 127)\n",
    "    return color.lab2rgb(lab)\n",
    "\n",
    "print(\"Upload a grayscale/B&W image to colorize...\")\n",
    "uploaded = files.upload()\n",
    "if uploaded:\n",
    "    name = list(uploaded.keys())[0]\n",
    "    print(\"Processing:\", name)\n",
    "    try:\n",
    "        rgb = pil_to_rgb_array_train_style(uploaded[name], TARGET_SIZE)\n",
    "        L = rgb_to_L(rgb)  # [0,100]\n",
    "\n",
    "        # Forward with safe pad/crop\n",
    "        ab = run_net_on_Lnorm_safe(L_to_model_input(L), stride=MODEL_STRIDE)  # (2,H,W) in [-1,1]\n",
    "\n",
    "        # Optional: smoothing + mild color boost + L contrast\n",
    "        if EDGE_AWARE_SMOOTHING:\n",
    "            ab = edge_aware_smooth_ab(L, ab, SMOOTH_SIGMA_SPATIAL, SMOOTH_SIGMA_LUMA)\n",
    "        ab = apply_ab_color_boost(ab, GLOBAL_AB_GAIN, CONF_WEIGHTED_BOOST, CONF_EPS)\n",
    "        L_adj = apply_l_adjustments(L)\n",
    "\n",
    "        out = lab_from_L_and_ab(L_adj, ab)\n",
    "\n",
    "        # Show\n",
    "        plt.figure(figsize=(14,6))\n",
    "        plt.subplot(1,2,1); plt.imshow(L/100.0, cmap='gray'); plt.title('Input (Grayscale)', fontweight='bold'); plt.axis('off')\n",
    "        plt.subplot(1,2,2); plt.imshow(out); plt.title('Colorized Output', fontweight='bold'); plt.axis('off')\n",
    "        plt.tight_layout(); plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error:\", str(e))\n",
    "else:\n",
    "    print(\"No file uploaded.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
